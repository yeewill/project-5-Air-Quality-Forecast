{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3091, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dskey = pickle.load(open(\"darkskyapi.p\",'rb'))\n",
    "open(\"darkskyapi.p\",'rb').close\n",
    "\n",
    "date_vars = ['Date']\n",
    "air_df = []\n",
    "for file in glob('ozone data by year/*.csv'):\n",
    "    air_df.append(pd.read_csv(file, parse_dates=date_vars, date_parser=pd.to_datetime))\n",
    "\n",
    "\n",
    "air_df = pd.concat(air_df)\n",
    "air_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dark sky single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long = air_df.iloc[299,-1]\n",
    "\n",
    "# lat =air_df.iloc[299,-2]\n",
    "\n",
    "# day = air_df.iloc[298,0]\n",
    "\n",
    "\n",
    "# day = int(time.mktime(day.timetuple()))\n",
    "# url ='https://api.darksky.net/forecast/'+dskey+'/'+str(lat)+','+str(long)+','+str(day)\n",
    "# weather= requests.get(url)\n",
    "# weather.json()['daily']['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_length =[]\n",
    "temp_high =[]\n",
    "temp_low =[]\n",
    "dew_point =[]\n",
    "humidity =[]\n",
    "pressure =[]\n",
    "wind_speed =[]\n",
    "wind_bearing =[]\n",
    "cloud_cover =[]\n",
    "uv_index =[]\n",
    "\n",
    "for i in range(air_df.shape[0]):\n",
    "    try:\n",
    "        long = air_df.iloc[i,-1]\n",
    "        lat =air_df.iloc[i,-2]\n",
    "        day = air_df.iloc[i,0]\n",
    "        day = pd.to_datetime(day , format ='%m/%d/%Y')\n",
    "        day = int(time.mktime(day.timetuple()))\n",
    "        url ='https://api.darksky.net/forecast/'+dskey+'/'+str(lat)+','+str(long)+','+str(day)\n",
    "        weather= requests.get(url)\n",
    "        day_length.append(weather.json()['daily']['data'][0]['sunsetTime'] - weather.json()['daily']['data'][0]['sunriseTime'])\n",
    "        try:\n",
    "            temp_high.append(weather.json()['daily']['data'][0]['temperatureHigh'])\n",
    "        except:\n",
    "            temp_high.append(weather.json()['daily']['data'][0]['temperatureMax'])\n",
    "        try:\n",
    "            temp_low.append(weather.json()['daily']['data'][0]['temperatureLow'])\n",
    "        except:\n",
    "            temp_low.append(weather.json()['daily']['data'][0]['temperatureMin'])\n",
    "        dew_point.append(weather.json()['daily']['data'][0]['dewPoint'])\n",
    "        humidity.append(weather.json()['daily']['data'][0]['humidity'])\n",
    "        pressure.append(weather.json()['daily']['data'][0]['pressure'])\n",
    "        wind_speed.append(weather.json()['daily']['data'][0]['windSpeed'])\n",
    "        wind_bearing.append(weather.json()['daily']['data'][0]['windBearing'])\n",
    "        cloud_cover.append(weather.json()['daily']['data'][0]['cloudCover'])\n",
    "        uv_index.append(weather.json()['daily']['data'][0]['uvIndex'])\n",
    "    except:\n",
    "        print('broken at: ' +str(i))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['day_length'] = day_length\n",
    "air_df['temp_high'] = temp_high\n",
    "air_df['temp_low'] = temp_low\n",
    "air_df['dew_point'] = dew_point\n",
    "air_df['humidity'] = humidity\n",
    "air_df['pressure'] = pressure\n",
    "air_df['wind_speed'] = wind_speed\n",
    "air_df['wind_bearing'] = wind_bearing\n",
    "air_df['cloud_cover'] = cloud_cover\n",
    "air_df['uv_index'] = uv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(air_df, open('air_df_1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df = pickle.load(open(\"air_df_1.p\",'rb'))\n",
    "open('air_df_1.p','rb').close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =air_df[['Date','Daily Max 8-hour Ozone Concentration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.query('Date < \"2018-07-01\"')\n",
    "valid = df.query('Date >= \"2018-07-01\" and Date < \"2018-11-01\"')\n",
    "test = df.query('Date >= \"2018-11-02\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save column names and indices to use when storing as csv\n",
    "cols = train.columns\n",
    "train_idx = train.index\n",
    "valid_idx = valid.index\n",
    "test_idx = test.index\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train = scaler.fit_transform(train)\n",
    "valid = scaler.transform(valid)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, title):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Nb Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    val_loss = history.history['val_loss']\n",
    "    min_idx = np.argmin(val_loss)\n",
    "    min_val_loss = val_loss[min_idx]\n",
    "    print('Minimum validation loss of {} reached at epoch {}'.format(min_val_loss, min_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lag = 14\n",
    "\n",
    "train_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)\n",
    "valid_data_gen = TimeseriesGenerator(train, train, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)\n",
    "test_data_gen = TimeseriesGenerator(test, test, length=n_lag, sampling_rate=1, stride=1, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn = Sequential()\n",
    "simple_rnn.add(SimpleRNN(5, input_shape=(n_lag, 1)))\n",
    "simple_rnn.add(Dense(1))\n",
    "simple_rnn.compile(loss='mae', optimizer=RMSprop())\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='simple_rnn_weights.hdf5'\n",
    "                               , verbose=0\n",
    "                               , save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss'\n",
    "                             , patience=10\n",
    "                             , verbose=0)\n",
    "with open(\"simple_rnn.json\", \"w\") as m:\n",
    "     m.write(simple_rnn.to_json())\n",
    "\n",
    "simple_rnn_history = simple_rnn.fit_generator(train_data_gen\n",
    "                                              , epochs=100\n",
    "                                              , validation_data=valid_data_gen\n",
    "                                              , verbose=2\n",
    "                                              , callbacks= [checkpointer ,earlystopper])\n",
    "plot_loss(simple_rnn_history, 'SimpleRNN - Train & Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm = Sequential()\n",
    "simple_lstm.add(LSTM(5, input_shape=(n_lag, 1)))\n",
    "simple_lstm.add(Dense(1))\n",
    "simple_lstm.compile(loss='mae', optimizer=RMSprop())\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='simple_lstm_weights.hdf5'\n",
    "                               , verbose=0\n",
    "                               , save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss'\n",
    "                             , patience=10\n",
    "                             , verbose=0)\n",
    "with open(\"simple_lstm.json\", \"w\") as m:\n",
    "    m.write(simple_lstm.to_json())\n",
    "\n",
    "simple_lstm_history = simple_lstm.fit_generator(train_data_gen\n",
    "                                                , epochs=100\n",
    "                                                , validation_data=valid_data_gen\n",
    "                                                , verbose=2                                           \n",
    "                                                , callbacks=[checkpointer, earlystopper])\n",
    "plot_loss(simple_lstm_history, 'Simple LSTM - Train & Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm = Sequential()\n",
    "stacked_lstm.add(LSTM(16, input_shape=(n_lag, 1), return_sequences=True))\n",
    "stacked_lstm.add(LSTM(8, return_sequences=True))\n",
    "stacked_lstm.add(LSTM(4))\n",
    "stacked_lstm.add(Dense(1))\n",
    "stacked_lstm.compile(loss='mae', optimizer=RMSprop())\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='stacked_lstm_weights.hdf5'\n",
    "                               , verbose=0\n",
    "                               , save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss'\n",
    "                             , patience=10\n",
    "                             , verbose=0)\n",
    "with open(\"stacked_lstm.json\", \"w\") as m:\n",
    "    m.write(stacked_lstm.to_json())\n",
    "\n",
    "stacked_lstm_history = stacked_lstm.fit_generator(train_data_gen\n",
    "                                                  , epochs=100\n",
    "                                                  , validation_data=valid_data_gen\n",
    "                                                  , verbose=2\n",
    "                                                  , callbacks=[checkpointer, earlystopper])\n",
    "plot_loss(stacked_lstm_history, 'Stacked LSTM - Train & Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_length'] = day_length\n",
    "df['temp_high'] = temp_high\n",
    "df['temp_low'] = temp_low\n",
    "df['dew_point'] = dew_point\n",
    "df['humidity'] = humidity\n",
    "df['pressure'] = pressure\n",
    "df['wind_speed'] = wind_speed\n",
    "df['wind_bearing'] = wind_bearing\n",
    "df['cloud_cover'] = cloud_cover\n",
    "df['uv_index'] = uv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df,open('ts_with_weather.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
